[{"authors":["admin"],"categories":null,"content":"Hengzhao Hong is a Bachelor of Economics(BEc) graduated from the School of Economics at Xiamen University. His research interests include applied econometrics, agricultural economics and statistical learning. Also, he is a big fan of programming. With the knowledge of Python and JavaScript, he has developed several scrapies, websites and data visualization systems according to his teacher\u0026rsquo;s or his economic reseach needs.\nAlthough his major is economics, by his interest and in his out-of-class time, he has been qualified and doing a job of front-end engineering as an intren for about a year. He loves designing and attaches great importance to the sense of beauty. He has an \u0026ldquo;OCD\u0026rdquo; of LaTeX when writing.\nRecently, after receieving his bachelor degree of Economics, he keeps learning.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"598b63dd58b43bce02403646f240cd3c","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"author","summary":"Hengzhao Hong is a Bachelor of Economics(BEc) graduated from the School of Economics at Xiamen University. His research interests include applied econometrics, agricultural economics and statistical learning. Also, he is a big fan of programming. With the knowledge of Python and JavaScript, he has developed several scrapies, websites and data visualization systems according to his teacher\u0026rsquo;s or his economic reseach needs.\nAlthough his major is economics, by his interest and in his out-of-class time, he has been qualified and doing a job of front-end engineering as an intren for about a year.","tags":null,"title":"Hengzhao Hong","type":"author"},{"authors":["Hengzhao Hong"],"categories":["Scrapy"],"content":" 近日某深夜，一位在远方实习的舍友来请求我的帮助\u0026hellip;他需要从一个网站上搜集近两年总共5万多条的基金季报数据（该数据属于公开数据），并把数据写入 Excel 文件保存。这是一个非常非常简单、基础的爬虫项目，很适合入门。\n 什么是 Python 爬虫：\n简单地说，爬虫就是用程序代替人工来访问大量结构相似的网页，解析这些网页的内容，提取出自己想要的数据，最后并把收集到的数据储存到本地，用于进一步的研究分析。以上这些工作全部由编写好的 Python 程序自动化地完成。\n爬虫可以节省大量人力时间，完成人力所不能及的大规模网络数据采集任务。比如：\n 访问一个信息公示网页内的几百页、上万个链接，从每个子链接点进去的表格中提取信息，形成一张汇总表格。 下载关于某个主题的几百万张图片到本地的一个指定文件夹内。 搜索、下载满足某个条件的一系列视频到本地。 收集相同商品在不同市场上的价格，并实时更新。  BTW, 舍友先拿着网站去问了淘宝代写爬虫的店家，这么简单的一个爬虫，店家居然要价 200\u0026hellip;我也有点心动实在有点看不过去，钱有这么好赚吗\u0026hellip;\n 爬虫需谨慎，本文的代码和数据都仅用于编程技术学习，请勿用于其他任何用途，否则造成的后果与本文作者无关。\n前提条件 在真正编写一个爬虫之前，我们需要 Python 环境。这里强烈推荐直接安装 Anaconda (Python 3+)，它集成了 Python 环境、用于数据科学的 Python 库和一系列非常实用的工具。\n安装 Anaconda 从 Anaconda 下载页面安装 Anaconda。安装时有一步会询问是否将 python 加入环境变量（PATH），在那里请勾选“是”，继续安装。\n安装完成后，命令行输入 python 并回车，会出现类似下面的提示，说明环境搭建成功。\nPython 3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)] :: Anaconda, Inc. on win32 Type \u0026quot;help\u0026quot;, \u0026quot;copyright\u0026quot;, \u0026quot;credits\u0026quot; or \u0026quot;license\u0026quot; for more information. \u0026gt;\u0026gt;\u0026gt;   如果此处报错，说明没有把 python 添加到系统环境变量里，查找 \u0026ldquo;Anaconda 添加环境变量\u0026rdquo; 的相关教程即可，这里不再赘述。\n 你还需要一个可以解析网络请求的浏览器（比如谷歌浏览器） 爬虫的本质是替你上网冲浪模仿浏览器的行为访问网站，区别在于，得到服务器响应之后，爬虫和人工访问用的浏览器做的事不一样：\n 人工访问网站时，浏览器做的事：浏览器会向网站的服务器发送请求，得到服务器返回的文件，然后在浏览器中解析这些文件，形成内容、版式，最后构成一个网页展示给我们看。 程序访问网站时，程序所做的事：程序模仿浏览器，向网站的服务器发送和浏览器一模一样的请求，得到一模一样的返回文件，然后在程序中直接解析这些文件，直接提取我们想要的内容、数据，把它们储存到本地。  显然，在一开始，我们的爬虫并不是一个成熟的爬虫，它需要我们代码的指引。为此，我们首先要知道浏览器请求这个网站时是怎么做的，才能让 Python 爬虫模仿它。\n在谷歌浏览器中，点击右键-检查（或者按 F12）进入浏览器控制台，在控制台中选择 Network 签页，按红点旁边的禁止图案清空已有记录，再访问我们要爬的网页，就可以读取到浏览器的发送的请求，以及服务器返回的具体文件了。其他浏览器有类似功能即可。\n\r谷歌浏览器的控制台\r\r 分析网站：我们要爬什么？ 先分析网站很重要，可以帮我们发现最合适、高效的方法。\n\r网站页面概览\r\r 从这里打开我们要爬取的网站，我们可以看到，页面的表格就是我们想要获取的基金季报数据，默认只显示 2019 年第一季度的数据。我们通过筛选按钮，筛选出从 2018 年第一季度至 2019 年第一季度的所有基金季报数据（共 58169 条）。\n\r筛选我们需要的时间区间\r\r 现在，网站页面显示，我们要爬取的表格有 2300 多页，每一页有 20 多行。\n要想把这些数据全部收集到 Excel 表格中，用人工显然是不可能的\u0026hellip;\u0026hellip;爬虫的优势这时就体现出来啦！\n通过浏览器捕获请求 在设计任何爬虫时，我们首先都想尝试知道浏览器如何请求到新数据的，如果知道这一点，爬虫只需要构造一模一样的请求，就可以拿到数据，这是最令人开心的简单的爬虫情形。\n如何捕获到这个关键的请求呢？一个简单的方法：现代的网站开发中，为了提高用户体验，在需要更新某些内容时，网页往往只会更新 需要更新的那部分 ，其他部分不会更新。因此，我们让这个网站面临一个只需要更新数据的情形 \u0026mdash;\u0026mdash; 翻页。\n在谷歌浏览器中，点击右键-检查（或者按 F12）进入浏览器控制台，在控制台中选择 Network 签页，按红点旁边的禁止图案清空已有记录，然后翻页。我们十分欣慰地发现，在翻页的过程中，浏览器只向服务器发送了唯一一条请求，因此我们基本可以确定，新一页的数据就在这条新请求的返回结果中。\n\r捕获翻页时发生的请求\r\r 点击该请求，\n Headers 签页描述了浏览器向哪个地址请求数据、传递的参数是什么，以及服务器如何响应该请求。 Preview 签页展示了服务器返回的内容  我们选择 Preview 页签，看到的是一个结构化的数据，展开其中 rows 字段，可以看到里面就是我们在这一页想要的数据。\n\r服务器返回的内容\r\r 找到关键请求之后，我们选择 Headers 页签，找到请求的 URL 和请求的方式（GET）。\n\r找到请求的 URL 和方式\r\r 在 Headers 页签的最下方找到请求的参数。这个参数是被传递给服务器，服务器可以根据参数返回正确的数据。对照参数和当前网页的内容，可以看出：\n rows 是每一页的行数 page 是当前页码（连续翻页观察请求的网址，也可以看出） sord 是排序顺序（升序/降序） sidx 是排序依据的字段名 quarter_id[] 则是之前选择的各个时间范围。  \r请求的参数\r\r 服务器根据这些参数查询数据，然后把数据返回给浏览器（这里被我们捕获），浏览器把数据填充进网页中，更新新一页的表格，完成翻页。整个请求的逻辑十分清晰，爬虫写起来也就十分容易。\n编写爬虫 新建文件 web_scrapy.py，用编辑器打开。\n 没有安装编辑器的话，强烈推荐安装 VS Code，一个免费、开源又功能强大的编辑器，你不会后悔的。\n 首先引入我们需要用到的三个库，\n# web_scrapy.py import time # 时间库，这里用于限制爬取的速度，否则程序的请求过快，可能影响到对方服务器的正常运行 import requests # 访问互联网的库，这里用于模拟浏览器发送请求，并接受返回数据 import pandas as pd # 数据处理库，这里用于将数据写入 Excel  设定爬虫的配置，后面会引用到这些变量。配置写在最前面，方便整个爬虫的维护。\nspeed_rate = 2 # 爬虫速度。这里把速度限制在人工访问的水平（每 2 秒请求一次），尽量避免影响服务器的正常运行 row_per_page = 100 # 每页请求 100 行，经过尝试，这是一次请求能得到的最大行数，默认是每页 25 行，我们这样设置能极大提升爬虫效率，也使爬取速度能被限制成更慢的速度，避免影响对方服务器的正常运行  尝试请求第一个页面，\n# web_scrapy.py res = requests.get('https://whalewisdom.com/dashboard/stock_screener_query?rows=%s\u0026amp;page=1\u0026amp;sidx=score\u0026amp;sord=desc\u0026amp;or_filers=0\u0026amp;quarter_id[]=73\u0026amp;quarter_id[]=72\u0026amp;quarter_id[]=71\u0026amp;quarter_id[]=70\u0026amp;quarter_id[]=69' % (row_per_page)) records = res.json()   requests.get() 函数里传入的是请求的目标 URL，我们从刚才捕获的请求中得到了单个页面的 URL，该函数使用 GET 方式向指定的 URL 发送网络请求 URL 中，参数 page=1，我们请求的是第一页的数据 URL 中，参数 rows=%s，使用了格式化字符串，将百分号后面的 row_per_page 变量输出到 %s 位置，从而配置了每页请求的行数。若看不懂该行语法，请复习 Python 格式化字符串。 requests.get() 返回的结果是一个 Response 对象，调用它的 json() 方法，我们得到 json 格式的返回数据，从浏览器控制台中该请求的 Preview 页签看得更美观一些。  检查请求是否成功：、\n# 打印请求状态码，200说明请求成功，404说明找不到页面，等等 print(res.status_code)  200   请求状态码的含义速记：以 2 开头的状态码意味着请求成功，以 4 或 5 开头的状态码意味着请求失败，详情可参考 W3C’s Status Code Definitions。\n print(records)  {'total': 582, 'facets': {'market_cap': [{'value': 1, 'count': 5916, 'is_true': True}, {'value': 2, 'count': 6792, 'is_true': True}, {'value': 3, 'count': 3370, 'is_true': True}, {'value': 4, 'count': 10731, 'is_true': True}, {'value': 5, 'count': 4249, 'is_true': True}, {'value': 6, 'count': 837, 'is_true': True}, {'value': 7, 'count': 711, 'is_true': True}], 'market_cap_13f': [{'value': 1, 'count': 36037, 'is_true': True}, {'value': 2, 'count': 7683, 'is_true': True}, {'value': 3, 'count': 3152, 'is_true': True}, {'value': 4, 'count': 8663, 'is_true': True}, {'value': 5, 'count': 2167, 'is_true': True}, {'value': 6, 'count': 289, 'is_true': True}, {'value': 7, 'count': 178, 'is_true': True}], 'turnover_13f': [{'value': 1, 'count': 11378, 'is_true': True}, {'value': 2, 'count': 768, 'is_true': True}, {'value': 3, 'count': 13493, 'is_true': True}, {'value': 4, 'count': 22213, 'is_true': True}, {'value': 5, 'count': 6041, 'is_true': True}, {'value': 6, 'count': 1202, 'is_true': True}], 'ranking': [{'value': 1, 'count': 280, 'is_true': True}, {'value': 2, 'count': 387, 'is_true': True}, {'value': 3, 'count': 717, 'is_true': True}, {'value': 4, 'count': 2142, 'is_true': True}, {'value': 5, 'count': 23239, 'is_true': True}, {'value': 6, 'count': 31376, 'is_true': True}], 'pe': [{'value': 1, 'count': 58169, 'is_true': True}], 'pe_next_year': [{'value': 1, 'count': 58160, 'is_true': True}, {'value': 3, 'count': 5, 'is_true': True}, {'value': 4, 'count': 4, 'is_true': True}], 'price_sales': [{'value': 1, 'count': 58169, 'is_true': True}], 'price_book': [{'value': 1, 'count': 58169, 'is_true': True}], 'dividend_yield': [], 'held_by': [{'value': 1, 'count': 23428, 'is_true': True}, {'value': 2, 'count': 6744, 'is_true': True}, {'value': 3, 'count': 5776, 'is_true': True}, {'value': 4, 'count': 6298, 'is_true': True}, {'value': 5, 'count': 15223, 'is_true': True}, {'value': 6, 'count': 700, 'is_true': True}], 'filer_ids': [], 'percent_change_in_shares_qoq': [{'value': 1, 'count': 4694, 'is_true': True}, {'value': 2, 'count': 14889, 'is_true': True}, {'value': 3, 'count': 4335, 'is_true': True}, {'value': 4, 'count': 2435, 'is_true': True}, {'value': 5, 'count': 1501, 'is_true': True}, {'value': 6, 'count': 2229, 'is_true': True}, {'value': 7, 'count': 16354, 'is_true': True}, {'value': 8, 'count': 4097, 'is_true': True}, {'value': 9, 'count': 2374, 'is_true': True}, {'value': 10, 'count': 2182, 'is_true': True}], 'percent_change_in_filers_qoq': [{'value': 1, 'count': 14783, 'is_true': True}, {'value': 2, 'count': 10969, 'is_true': True}, {'value': 3, 'count': 5392, 'is_true': True}, {'value': 4, 'count': 2806, 'is_true': True}, {'value': 5, 'count': 1765, 'is_true': True}, {'value': 6, 'count': 545, 'is_true': True}, {'value': 7, 'count': 11159, 'is_true': True}, {'value': 8, 'count': 4832, 'is_true': True}, {'value': 9, 'count': 2305, 'is_true': True}, {'value': 10, 'count': 539, 'is_true': True}], 'quarter_id': [{'value': 71, 'count': 11767, 'is_true': True}, {'value': 72, 'count': 11701, 'is_true': True}, {'value': 73, 'count': 11611, 'is_true': True}, {'value': 69, 'count': 11588, 'is_true': True}, {'value': 70, 'count': 11502, 'is_true': True}], 'sector': [{'value': 'FINANCE', 'count': 18660, 'is_true': True}, {'value': 'UNKNOWN', 'count': 7284, 'is_true': True}, {'value': 'HEALTH CARE', 'count': 4859, 'is_true': True}, {'value': 'INFORMATION TECHNOLOGY', 'count': 3745, 'is_true': True}, {'value': 'INDUSTRIALS', 'count': 3502, 'is_true': True}, {'value': 'CONSUMER DISCRETIONARY', 'count': 3447, 'is_true': True}, {'value': 'MUTUAL FUND', 'count': 3406, 'is_true': True}, {'value': 'MATERIALS', 'count': 2867, 'is_true': True}, {'value': 'ENERGY', 'count': 2630, 'is_true': True}, {'value': 'REAL ESTATE', 'count': 2372, 'is_true': True}, {'value': 'CONSUMER STAPLES', 'count': 1622, 'is_true': True}, {'value': 'UTILITIES AND TELECOMMUNICATIONS', 'count': 1556, 'is_true': True}, {'value': 'COMMUNICATIONS', 'count': 1306, 'is_true': True}, {'value': 'TRANSPORTS', 'count': 733, 'is_true': True}, {'value': 'UTILITIES', 'count': 76, 'is_true': True}, {'value': 'Communications', 'count': 60, 'is_true': True}, {'value': 'Utilities', 'count': 22, 'is_true': True}, {'value': '', 'count': 10, 'is_true': True}, {'value': 'Consumer Staples', 'count': 5, 'is_true': True}, {'value': 'Transports', 'count': 4, 'is_true': True}, {'value': 'Health Care', 'count': 3, 'is_true': True}], 'industry': []}, 'records': 58169, 'page': 1, 'sort': 'score', 'dir': 'desc', 'rows': [{'id': '1470482', 'score': None, 'permalink': 'aeis', 'symbol': 'AEIS', 'name': 'ADVANCED ENERGY INDUSTRIES INC', 'held_by': 292, 'market_cap': 2306173000.0, 'market_cap_13f': 2132144670.0, 'dividend_yield': None, 'pe': None, 'turnover': 28.343948, 'percent_change_in_mv_qoq': -8.273920042454789, 'percent_change_in_shares_qoq': 0.8444298, 'percent_change_in_filers_qoq': -7.0063696, 'pe_next_year': None, 'average_ranking': 902, 'quarter': 'Q2 2018', 'sector': 'INFORMATION TECHNOLOGY', 'industry': 'SEMICONDUCTORS'}, ...]}  返回的 json 里（从浏览器请求的 Preview 页签中看），records 字段是总数据量，rows 是每页请求的数据量，total 字段是根据 rows 的总页数，利用这些信息，我们可以估计爬取时间，从而合理地调整速度和每页请求量。\n 由于学习的目的，爬取速度在允许的情况下尽量限制得慢一点，避免影响对方服务器正常运行。\n print('总数据量：%s' % records['records']) print('每页数据量：%s' % len(records['rows'])) min_request_time = records['records']//len(records['rows']) + 1 print('最少请求次数：%s 次' % (min_request_time)) print('爬取速度设定为：每 %.2f 秒请求一次' % speed_rate) expected_time_spend = min_request_time*speed_rate print('预计耗时：%.2f 秒 = %.2f 分钟 = %.2f 小时' % (expected_time_spend, expected_time_spend/60, expected_time_spend/3600))   若以上语法看不懂，请复习 Python 格式化字符串。\n 输出结果：\n总数据量：58169 每页数据量：100 最少请求次数：582 次 爬取速度设定为：每 2.00 秒请求一次 预计耗时：1164.00 秒 = 19.40 分钟 = 0.32 小时  之前，连续翻页时分析请求，我们发现 URL 中只有 page 参数发生了变化。因此，要想爬取全部数据，我们只需写一个循环，每次循环改变 page 参数值，就可以爬下全部页面的数据了。\n# 定义一个空数组，用于存放爬下来的全部数据 result_data = [] # 循环爬取每一个页面，数据写入刚才的空数组中 for i in range(min_request_time): # 使用格式化字符串的语法，先生成 URL url = 'https://whalewisdom.com/dashboard/stock_screener_query?rows=%s\u0026amp;page=%s\u0026amp;sidx=score\u0026amp;sord=desc\u0026amp;or_filers=0\u0026amp;quarter_id[]=73\u0026amp;quarter_id[]=72\u0026amp;quarter_id[]=71\u0026amp;quarter_id[]=70\u0026amp;quarter_id[]=69' % (row_per_page, i+1) # 发起请求，打印请求的状态和 URL。这样爬虫运行的时候可以把握进度。 res = requests.get(url) print('%s %s' % (res.status_code, res.url)) records = res.json() # 数据就在每个 json 返回结果的 rows 字段中，前面已分析。 data = records['rows'] # 重命名变量，并把结果追加写入数组中 for row in data: result_data.append({ 'Quarter': row['quarter'], 'Name': row['name'], 'Ticker': row['symbol'], 'Sector': row['sector'], 'Industry': row['industry'], '# Held_By': row['held_by'], 'Overall_Market_Cap': row['market_cap'], '13F Market Cap ': row['market_cap_13f'], 'QoQ % Change in 13F Market Cap': row['percent_change_in_mv_qoq'], '13F Turnover ': row['turnover'], 'Avg 13F Ranking ': row['average_ranking'], '% Change in Filers': row['percent_change_in_filers_qoq'], '% Change in Shares': row['percent_change_in_shares_qoq'] }) # 十分重要，让程序在指定时间后才继续进入下一次循环，这里是 2 秒，speed_rate = 2 # 限制了爬虫请求速度最快为 2 秒一次 time.sleep(speed_rate)  爬虫项目的网络数据采集部分到此完成，循环结束后，result_data 变量即为爬取得到的数据。接下来我们将数据写入 Excel 表格中。\n储存数据 写入 Excel 也很简单，使用开头引入的 pandas 库即可。\n# 定义变量在表格中展示的顺序 result_order = [ 'Quarter', 'Name', 'Ticker', 'Sector', 'Industry', '# Held_By', 'Overall_Market_Cap', '13F Market Cap ', 'QoQ % Change in 13F Market Cap', '13F Turnover ', 'Avg 13F Ranking ', '% Change in Filers', '% Change in Shares' ] # 使用 result_data 数组生成数据表格 DataFrame 对象，这里 pd 来自最开始引入的 pandas 库 result_df = pd.DataFrame.from_records(data = result_data) # 调整变量在表格中展示的顺序 result_df = result_df[result_order] # 将 DataFrame 对象写入 Excel 文件中，文件名为 data.xlsx，Sheet 名为 data result_df.to_excel(\u0026quot;data.xlsx\u0026quot;, sheet_name='data', index = False)   若在 Jupyter Notebook 中运行，直接执行 result_df 也可以看到最终的数据表。\n 完整代码 # web_scrapy.py import time # 时间库，这里用于限制爬取的速度，否则程序的请求过快，可能影响到对方服务器的正常运行 import requests # 访问互联网的库，这里用于模拟浏览器发送请求，并接受返回数据 import pandas as pd # 数据处理库，这里用于将数据写入 Excel # 配置 speed_rate = 2 row_per_page = 100 # 预先请求一次，估计爬虫时间等信息 res = requests.get('https://whalewisdom.com/dashboard/stock_screener_query?rows=%s\u0026amp;page=1\u0026amp;sidx=score\u0026amp;sord=desc\u0026amp;or_filers=0\u0026amp;quarter_id[]=73\u0026amp;quarter_id[]=72\u0026amp;quarter_id[]=71\u0026amp;quarter_id[]=70\u0026amp;quarter_id[]=69' % (row_per_page)) records = res.json() # print(res.status_code) # print(records) print('总数据量：%s' % records['records']) print('每页数据量：%s' % len(records['rows'])) min_request_time = records['records']//len(records['rows']) + 1 print('最少请求次数：%s 次' % (min_request_time)) print('爬取速度设定为：每 %.2f 秒请求一次' % speed_rate) expected_time_spend = min_request_time*speed_rate print('预计耗时：%.2f 秒 = %.2f 分钟 = %.2f 小时' % (expected_time_spend, expected_time_spend/60, expected_time_spend/3600)) # 下面爬取全部页面的数据 # 定义一个空数组，用于存放爬下来的全部数据 result_data = [] # 循环爬取每一个页面，数据写入刚才的空数组中 for i in range(min_request_time): # 使用格式化字符串的语法，先生成 URL url = 'https://whalewisdom.com/dashboard/stock_screener_query?rows=%s\u0026amp;page=%s\u0026amp;sidx=score\u0026amp;sord=desc\u0026amp;or_filers=0\u0026amp;quarter_id[]=73\u0026amp;quarter_id[]=72\u0026amp;quarter_id[]=71\u0026amp;quarter_id[]=70\u0026amp;quarter_id[]=69' % (row_per_page, i+1) # 发起请求，打印请求的状态和 URL。这样爬虫运行的时候可以把握进度。 res = requests.get(url) print('%s %s' % (res.status_code, res.url)) records = res.json() # 数据就在每个 json 返回结果的 rows 字段中，前面已分析。 data = records['rows'] # 重命名变量，并把结果追加写入数组中 for row in data: result_data.append({ 'Quarter': row['quarter'], 'Name': row['name'], 'Ticker': row['symbol'], 'Sector': row['sector'], 'Industry': row['industry'], '# Held_By': row['held_by'], 'Overall_Market_Cap': row['market_cap'], '13F Market Cap ': row['market_cap_13f'], 'QoQ % Change in 13F Market Cap': row['percent_change_in_mv_qoq'], '13F Turnover ': row['turnover'], 'Avg 13F Ranking ': row['average_ranking'], '% Change in Filers': row['percent_change_in_filers_qoq'], '% Change in Shares': row['percent_change_in_shares_qoq'] }) # 十分重要，让程序在指定时间后才继续进入下一次循环，这里是 2 秒，speed_rate = 2 # 限制了爬虫请求速度最快为 2 秒一次 time.sleep(speed_rate) # 定义变量在表格中展示的顺序 result_order = [ 'Quarter', 'Name', 'Ticker', 'Sector', 'Industry', '# Held_By', 'Overall_Market_Cap', '13F Market Cap ', 'QoQ % Change in 13F Market Cap', '13F Turnover ', 'Avg 13F Ranking ', '% Change in Filers', '% Change in Shares' ] # 使用 result_data 数组生成数据表格 DataFrame 对象，这里 pd 来自最开始引入的 pandas 库 result_df = pd.DataFrame.from_records(data = result_data) # 调整变量在表格中展示的顺序 result_df = result_df[result_order] # 将 DataFrame 对象写入 Excel 文件中，文件名为 data.xlsx，Sheet 名为 data result_df.to_excel(\u0026quot;data.xlsx\u0026quot;, sheet_name='data', index = False)  运行爬虫与成果 运行该 py 文件即可。\n 相同目录下打开命令行，执行 python web_scrapy.py 即可 也可以将代码复制进 Jupyter Notebook 运行 可以使用 VS Code 运行和调试该文件  输出结果如下示例，每2秒会发起一个新请求，因而每两秒打印一条请求记录，作为爬虫日志。\n总数据量：58169 每页数据量：100 最少请求次数：582 次 爬取速度设定为：每 2.00 秒请求一次 预计耗时：1164.00 秒 = 19.40 分钟 = 0.32 小时 200 https://whalewisdom.com/dashboard/stock_screener_query?rows=100\u0026amp;page=1\u0026amp;sidx=score\u0026amp;sord=desc\u0026amp;or_filers=0\u0026amp;quarter_id[]=73\u0026amp;quarter_id[]=72\u0026amp;quarter_id[]=71\u0026amp;quarter_id[]=70\u0026amp;quarter_id[]=69 200 https://whalewisdom.com/dashboard/stock_screener_query?rows=100\u0026amp;page=2\u0026amp;sidx=score\u0026amp;sord=desc\u0026amp;or_filers=0\u0026amp;quarter_id[]=73\u0026amp;quarter_id[]=72\u0026amp;quarter_id[]=71\u0026amp;quarter_id[]=70\u0026amp;quarter_id[]=69 200 https://whalewisdom.com/dashboard/stock_screener_query?rows=100\u0026amp;page=3\u0026amp;sidx=score\u0026amp;sord=desc\u0026amp;or_filers=0\u0026amp;quarter_id[]=73\u0026amp;quarter_id[]=72\u0026amp;quarter_id[]=71\u0026amp;quarter_id[]=70\u0026amp;quarter_id[]=69 200 https://whalewisdom.com/dashboard/stock_screener_query?rows=100\u0026amp;page=4\u0026amp;sidx=score\u0026amp;sord=desc\u0026amp;or_filers=0\u0026amp;quarter_id[]=73\u0026amp;quarter_id[]=72\u0026amp;quarter_id[]=71\u0026amp;quarter_id[]=70\u0026amp;quarter_id[]=69 200 https://whalewisdom.com/dashboard/stock_screener_query?rows=100\u0026amp;page=5\u0026amp;sidx=score\u0026amp;sord=desc\u0026amp;or_filers=0\u0026amp;quarter_id[]=73\u0026amp;quarter_id[]=72\u0026amp;quarter_id[]=71\u0026amp;quarter_id[]=70\u0026amp;quarter_id[]=69 200 https://whalewisdom.com/dashboard/stock_screener_query?rows=100\u0026amp;page=6\u0026amp;sidx=score\u0026amp;sord=desc\u0026amp;or_filers=0\u0026amp;quarter_id[]=73\u0026amp;quarter_id[]=72\u0026amp;quarter_id[]=71\u0026amp;quarter_id[]=70\u0026amp;quarter_id[]=69 200 https://whalewisdom.com/dashboard/stock_screener_query?rows=100\u0026amp;page=7\u0026amp;sidx=score\u0026amp;sord=desc\u0026amp;or_filers=0\u0026amp;quarter_id[]=73\u0026amp;quarter_id[]=72\u0026amp;quarter_id[]=71\u0026amp;quarter_id[]=70\u0026amp;quarter_id[]=69 200 https://whalewisdom.com/dashboard/stock_screener_query?rows=100\u0026amp;page=8\u0026amp;sidx=score\u0026amp;sord=desc\u0026amp;or_filers=0\u0026amp;quarter_id[]=73\u0026amp;quarter_id[]=72\u0026amp;quarter_id[]=71\u0026amp;quarter_id[]=70\u0026amp;quarter_id[]=69 200 https://whalewisdom.com/dashboard/stock_screener_query?rows=100\u0026amp;page=9\u0026amp;sidx=score\u0026amp;sord=desc\u0026amp;or_filers=0\u0026amp;quarter_id[]=73\u0026amp;quarter_id[]=72\u0026amp;quarter_id[]=71\u0026amp;quarter_id[]=70\u0026amp;quarter_id[]=69 ...  得到 Excel 数据文件（示例，可能顺序有所差别）：\n\rExcel 数据文件\r\r 结语 这是一个非常简单的爬虫，但一开始没有经过那些请求分析的话，入门可能依然毫无头绪，写的爬虫也会更为复杂。\n在更严谨的爬虫中，我们需要用 try...except...finally... 语法对请求失败的情况进行处理（比如重试请求等等），这样能避免因为一个小的请求错误导致整个爬虫中断。只需微调代码即可（关于该语法可复习这里）。\n在这个爬虫中，我们通过翻页的动作找到了浏览器请求新数据的关键请求，从而能直接构造这个请求得到数据。在另一些情况中，这个关键请求难以捕获，如果捕获不到数据生产的“过程”，那么我们只能从我们看到的最终网页“结果”出发，用程序（Python 的 BeautifulSoup4 库）来解析最终的 html 网页文件，从众多标签中分离出我们需要的数据储存起来，那种方法显然也能爬取本文所爬取的网站（下一篇入门教程就决定是它了），但它不比本文的这类方法高效（还需要额外处理该网站的动态加载问题），得到的数据精度（对比网页数据和请求的数据可以看出这一点）、运行效率也不如直接捕获“过程”的这类程序来得好，那种方法是本文这类方法行不通时的另一个通用策略，也仍是值得进一步学习的重要技术之一。\n","date":1560150184,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560150184,"objectID":"e8f48f975fa5773f63e991c0da7efdb3","permalink":"/post/a-very-very-simple-scrapy-using-python/","publishdate":"2019-06-10T15:03:04+08:00","relpermalink":"/post/a-very-very-simple-scrapy-using-python/","section":"post","summary":"Python in Practice","tags":["Python","ABC"],"title":"A Very Very Simple Scrapy Using Python","type":"post"},{"authors":["Hengzhao Hong"],"categories":["GitHub"],"content":" 本文介绍了如何在 Windows 上使用 Jekyll 框架搭建 GitHub Pages 个人博客。\n 使用 Jekyll + GitHub Pages ，可以在短时间内搭建出精美、个性的博客、项目网站或者线上简历等。我们只需要专注于编辑文章内容，将文章放入预设的模板（主题）中，再将全部文件提交到 GitHub 上，网站就会自动完成构建了。\n 使用 Jekyll 搭建的博客案例：\n Blog of Zach Holman  搭建 GitHub Pages - 极速尝鲜 如果你是零基础的话，现在用最快最简单的方法搭建一个 GitHub Page 吧！\n如果想更加个性化地使用 GitHub Page，可以直接跳过这一步。\n 本地编辑的技能可以尝鲜之后再慢慢补上嘛。\n 第一步：获取 Jekyll 模板，生成网站 如果没有 GitHub 账号，先在 GitHub 上注册一个。\n登录 GitHub，从以下 Jekyll 官方收录的主题中选择一个中意的样式，然后打开其 GitHub 项目界面。\n http://jekyllthemes.org/ https://jekyllthemes.io/  以 Kiko Plus 主题为例，点击 Homepage 找到项目主页。\n 也可以进入示例（Demo）页面，点击页面最下方的 GitHub 按钮进入项目主页。大多数主题的项目主页都可以这样找到。\n 进入 kiko-now 项目主页，根目录文件结构应该类似这样：\n点击 Fork 按钮，从而将这个库复制到自己的远程库中。\n进入自己的 kiko-now 库，在 Settings 里将库名改为 \u0026lt;你的用户名\u0026gt;.github.io，比如 honghzh.github.io。\n 此时，GitHub 会自动识别这个库，并基于库里的文件（也就是你刚刚“复制”的 Jekyll 主题模板文件）自动构建你的 GitHub Page。\n在许多主题项目的 README.md 中也会有使用说明。\n 等待片刻，直接访问 \u0026lt;你的用户名\u0026gt;.github.io，就可以看到自己的博客了。\n第二步：定制博客信息 在你的网站项目库中，打开根目录的 _config.yml，按照文件中的提示修改文件，把原来的博客信息（姓名、邮箱、github地址、文章网址格式等）改成你自己想要的配置信息，然后提交（Commit）即可。\n第三步：写文章 在你的网站项目库 _posts 文件夹中，新建一个 .md 文件，模仿原有的示例文件编辑文件头部（配置标题、发表时间等，常用配置规则），然后使用 Markdown 语言写文章（Markdown 简明语法规则），保存提交（Commit）即可。\n\r每篇文章的 markdown 文件都必须命名为 year-month-day-title.md 格式，这是 Jekyll 框架所要求的。\n\r\r等待片刻，直接访问 \u0026lt;你的用户名\u0026gt;.github.io ，会发现博客信息已经变味你自己的，而且文章也更新了。\n 如果文章没有出现，可能是文件命名有误，或者发表时间有误（或时区问题导致现在尚未到发表时间）。\n 现在你已经成功使用 Jekyll + GitHub Pages 搭建出个人网站了！\n如果你还想：\n 在本地用你喜欢的编辑器编辑文章 在本地实时预览文章效果和博客修改效果（本地预览比 GitHub 在线构建快得多） 在已有主题（或空白主题）的基础上，设计自己的博客样式 喜欢那种随心支配所有代码、掌控全局的感觉（比如我）  那么请看下面的完整版：\nJekyll + GitHub Pages 本地调试与线上搭建 在 GitHub 上的准备 首先，在 GitHub 上注册一个账号。\n 注意：接下来的步骤与前一节（极速尝鲜版）不同\n 登录 GitHub 后，访问 https://github.com/new 可以开始新建一个库（或点击顶部 + 号按钮 -\u0026gt; New repository），库名（Repository name）必须设为 \u0026lt;你的用户名\u0026gt;.github.io，选择库的类型为公共库（Public），从而创建一个新库，用于在 GitHub 上存放你的网站。\n 如果这个库已经存在，那么需要把原库先改名或删除。\n 本地调试所需的准备 因为 Jekyll 框架是基于 Ruby 语言构建的，所以，如果要在本地编辑和预览，你的计算机显然也要学会 Ruby 语言。\n安装 Ruby 在 Windows 系统安装 Ruby 很简单。直接访问 Ruby Installer for Windows下载安装程序，注意要下载 Ruby + DevKit 的集成版本，其中 DevKit 是 Ruby 的开发工具包，后续安装 Jekyll 框架时必须用到它。\n然后，一路安装即可，安装完成的最后会询问是否安装开发者工具（MSYS2 Devkit），选择“是”。\n安装 MSYS2 Devkit 时，会询问安装的范围，选择 1 或 3 安装整个开发工具包（MSYS2 toolchain）。\n修改 RubyGems 的默认下载源  Ruby 安装完毕后，需要使用 Ruby 自带的包管理工具（RubyGems）安装 Jekyll 包。\n RubyGems 默认下载源在国外，由于众所周知的原因，国内下载 Jekyll 等包的速度奇慢，因此需要先把下载源改为国内的官方镜像。\n打开命令窗口（可以按Shift后从右键菜单打开），依次执行：\ngem sources --add https://gems.ruby-china.com/ --remove https://rubygems.org/ gem sources -l # 将显示 https://gems.ruby-china.com # 确保只有 gems.ruby-china.com  修改完成。\n安装 Jekyll  因为我们要用 GitHub Page 构建网站，所以这里强烈推荐 GitHub Pages Gem 项目提供的 github-pages 包，它集成了 Jekyll 和 Jekyll 所需的所有依赖包（Dependencies），并且所有包都维护在 GitHub Page 支持的版本下。\n 推荐直接安装 github-pages 包（其中集成了 Jekyll），命令是：\ngem install github-pages  这样就可以避开很多的版本依赖和版本支持的问题。\n安装 Git  Git 是一个版本控制工具，Git 在这里用于关联和管理 GitHub 上刚才新建的远程库和本地的网站文件夹（本地库）。\nGit 是 GitHub 存在的基石。被 Git 工具所维护的文件夹叫做“库”（Repository），本地的文件夹就是本地库，在服务器上的文件夹就是远程库。GitHub 只是一个托管远程库，方便远程协作和储存版本的平台罢了。\n 从 Git 官网 上下载 Git 并安装即可。安装完成后，右键菜单会多出 Git Bash Here，点击后可以打开 Git 命令行，后续会在里面运行 git \u0026lt;命令\u0026gt;。\n安装文本编辑器  文本编辑器是一种编程工具，主流的编辑器有 NotePad++, Vs Code, Sublime Text 等，它们可以在编辑代码时提供高亮、提示、编译等功能。\n 如果本地没有文本编辑器，推荐安装VS Code，之后将使用文本编辑器打开/编辑本地的网站文件。\n本地调试 从以下 Jekyll 官方收录的主题中选择一个中意的样式，然后打开其 GitHub 项目界面。\n http://jekyllthemes.org/ https://jekyllthemes.io/  使用 git clone 命令将主题项目下载到本地。\n以 kiko-now 项目为例，首先在项目主页复制 git clone 的目标地址：\n然后在本地右键 - Git Bash Here，执行 git 命令：git clone \u0026lt;目标地址\u0026gt; 即可：\ngit clone https://github.com/aweekj/kiko-now.git  在项目根目录，命令行运行 jekyll serve，然后在浏览器地址栏中输入 localhost:4000 即可访问本地预览。\n在命令行窗口连续按 Ctrl+C ，再输入 y 确认退出，即可退出本地预览。\n配置博客信息、写文章 修改根目录的 _config.yml，即可配置自己的博客信息。注意，这个文件修改保存后，需要重新运行 jekyll serve 才能看到修改效果。\n在 _post 文件夹下新建 .md 文件，按原有示例文件的格式输入文件头部（配置标题、发表时间等，常用配置规则），然后使用 Markdown 语言写文章（Markdown 简明语法规则），保存即可。\n\r每篇文章的 markdown 文件都必须命名为 year-month-day-title.md 格式，这是 Jekyll 框架所要求的。\n\r\r文章编辑过程中，可以随时保存，并刷新 localhost:4000 查看最新的预览。\n编译 可选步骤：命令行 jekyll build 将编译网站文件，编译好的网站文件默认位于根目录的 _site 文件夹中。\n提交生成 GitHub Page 提交到 GitHub 上的两种形式：\n 可以直接将 _site 文件夹内的文件提交到 GitHub（需先用 jekyll build 编译）。 也可以将整个网站文件夹提交上去（此时无需用 jekyll build 编译）。  方法：\n 前期准备时，在 GitHub 上有一个名为 \u0026lt;你的用户名\u0026gt;.github.io 的库，现在将它 git clone 克隆到本地（在模板文件夹的外部），作为本地网站文件夹。 按上述两种形式中的一种，将所需文件复制进刚才 git clone 的文件夹内。 在本地网站文件夹内，右键 - git bash here，依次执行下列命令：\ngit add . git commit -m \u0026quot;\u0026lt;提交版本的名字，自定义\u0026gt;\u0026quot; git push origin master     如果复制文件到本地网站库时，你是将整个主题项目库复制进去的，那么以后可以直接在本地网站库中修改、预览，最后执行以上三条命令，即可将修改推送到自己的 GitHub 上。 如果只复制 _site 文件夹，那么需要在主题项目库中编辑、预览，然后重复上述方法来更新网站。   片刻之后，直接访问 \u0026lt;你的用户名\u0026gt;.github.io 即可查看你的博客网站，效果与本地预览相同（除某些特殊插件效果外）。\nWell done !\n","date":1553088895,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553088895,"objectID":"275c878723fe7c82db8971ecb400c6af","permalink":"/post/a-guidance-to-beautiful-jekyll-pages/","publishdate":"2019-03-20T21:34:55+08:00","relpermalink":"/post/a-guidance-to-beautiful-jekyll-pages/","section":"post","summary":"How to build a beautiful GitHub Page via Jekyll on Windows?","tags":["Jekyll","Blog"],"title":"A Guidance to Beautiful Jekyll Pages","type":"post"},{"authors":["Hengzhao Hong"],"categories":["Tools"],"content":" 本篇总结了 Git 使用过程中主要将用到的命令和其它一些问题的解决方法，包括：\n Git 分支管理 本地仓库-远程仓库的拉取/推送/关联 .gitignore 文件。 用户邮箱配置 SSH Key 配置  分支 创建 Git 分支，可以在原来的版本链上衍生出一条新的、独立的版本链，在新链上进行开发，再合并到主链中，这是协同开发和版本控制的精髓。\n增删分支 # 创建分支 git branch new_branch_name # 切换分支 git checkout branch_name # 创建分支，并切换到新分支 git checkout -b new_branch_name # 合并分支 git merge branch_name # 删除分支 git branch -d branch_name  查分支 # 检查分支版本 git log --oneline --decorate # 查看分支分叉历史 git log --oneline --decorate --graph --all  从另一个分支同步特定文件 git checkout \u0026lt;另一个分支名\u0026gt; \u0026lt;特定文件名\u0026gt;\n远程库推送、拉取 本节内容为如何关联本地库与远程库，将你的代码托管在网络上，作为备份或和合作者协同开发。\n远程库查询、增加、删除 # 查询 git remote -v # 增加 git remote add origin \u0026lt;你的项目地址\u0026gt; # 删除 git remote rm origin  推送到远程库 git push \u0026lt;远程主机名\u0026gt; \u0026lt;本地分支名\u0026gt;:\u0026lt;远程分支名\u0026gt;  首次推送并关联分支 git push -u origin master  从远程库拉取 git pull = git fetch + git merge\ngit pull \u0026lt;远程主机名\u0026gt; \u0026lt;远程分支名\u0026gt;:\u0026lt;本地分支名\u0026gt;  如果报错：本地和远程库是独立构建的，那么添加如下参数即可：\ngit pull origin master --allow-unrelated-histories  .gitignore 在 .gitignore 文件中声明的文件名，将不会被 Git 追踪（也就不会对它们进行版本控制、推送到远程库等），除非在更新 .gitignore 之前它们已经被追踪。\n.gitignore 只对未追踪的文件生效 解决方法 注意：该方法会将暂存区里面的文件也同时删除！\n在目标分支下，\ngit rm -r --cached \u0026lt;想重新忽略的文件名\u0026gt; git add . git commit -m \u0026quot;update gitignore\u0026quot;  设置 Git 用户名和邮箱 全局范围配置方法 git config --global user.name \u0026quot;github’s Name\u0026quot; git config --global user.email \u0026quot;github@xx.com\u0026quot; git config --list  只作用于当前项目的配置方法 git config user.name “gitlab’s Name” git config user.email \u0026quot;gitlab@xx.com\u0026quot; git config --list   也可以直接修改项目文件夹的 .git/config 文件，这个文件还可以配置 Git 的很多东西，包括关联的远程库地址、用户等。\n SSH Key SSH Key 是一个身份标识，用于让远程库（比如 GitHub）识别出你这台计算机的身份，从而不需要密码就可以安全提交。\n第一步：生成 public key 和 private key cd ~/.ssh ls # 此时会显示一些文件 mkdir key_backup cp id_rsa* key_backup rm id_rsa* # 以上三步为备份和移除原来的SSH key设置 ssh-keygen -t rsa -C \u0026quot;邮件地址@youremail.com\u0026quot; #生成新的key文件,邮箱地址填你的Github地址 # Enter file in which to save the key (/Users/your_user_directory/.ssh/id_rsa):\u0026lt;回车就好\u0026gt; #如果是二次创建，需要输入新的文件名 # 接下来会让你输入密码  第二步：查看 SSH 公钥 cd ~/.ssh ls # 会列出文件，里面包含公钥文件 vim id_rsa.pub # 用 vim 编辑器打开公钥文件 # 复制文件内容到 github 即可  第三步：测试 SSH ssh -T git@github.com # 之后会要你输入yes/no,输入yes就好  一台电脑多个 Git 账号，配置多个 SSH Key 的方法 第一步：创建 SSH KEY 同上\n第二步：配置 config 文件 查看用户~/.ssh下是否存在 config文件，如不存在使用命令 touch config创建，然后配置config\n# 配置github.com Host github.com HostName github.com IdentityFile C:\\\\Users\\\\popfisher\\\\.ssh\\\\id_rsa_github PreferredAuthentications publickey User username1 # 配置git.oschina.net Host git.oschina.net HostName git.oschina.net IdentityFile C:\\\\Users\\\\popfisher\\\\.ssh\\\\id_rsa_oschina PreferredAuthentications publickey User username2  第三步：测试 ssh -T git@git.oschina.net  如果配置正确会提示 Hi your git account two in github ! You've successfully authenticated, but GitHub does not provide shell access.\n使用方法：clone 项目到本地 原先操作是: git clone git@github.com:yourAccount/xxx.git\n现改为：git clone git@git.oschina.net:yourAccount/xxx.git\n","date":1550286142,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1550286142,"objectID":"ca71c7e56a5ef5f2b93facc48cf878fa","permalink":"/post/useful-commands-of-git/","publishdate":"2019-02-16T11:02:22+08:00","relpermalink":"/post/useful-commands-of-git/","section":"post","summary":"This article provides elegant ways to use Git.","tags":["Git"],"title":"Useful Commands of Git","type":"post"},{"authors":["Hengzhao Hong"],"categories":["Tools"],"content":" 本文是一篇入门文章，简单介绍了 Git、 GitHub 以及 GitHub Pages。\nGit Git 是一个版本控制工具。\n版本控制是什么意思？\n版本控制 比如，你想对电脑上某个文件夹进行版本控制，你首先会告诉 Git :\n\r“这个文件夹的一举一动，帮我盯紧了！”\n\r\r 具体实现：安装 Git（Git 官网），在项目文件夹中，右键 -\u0026gt; Git Bash Here，打开 Git 命令窗口，执行 git init 命令。\n Git 就会把这个文件夹看成一个 Git 仓库（repository）来照看它，仓库里多了什么、少了什么，都逃不过 Git 的火眼金睛。\n接着，你在这个仓库中删除了一些文件，又新建了一些文件，还修改了一些文件，告诉 Git：\n\r“这是这个库的一个新的版本了，记住它。”\n\r\r 具体实现：先暂存，后提交\n暂存版本：git add .\n提交新版本：git commit -m \u0026quot;\u0026lt;提交信息\u0026gt;\u0026quot;\n Git 就会在小本子上记下：\n\r“某年某月某日，主人提交了这个仓库（repository）的一个新版本，现在仓库里有这些文件，文件的内容分别是 XXX...”\n\r\r很多天过去了，你又作了很多改动，提交了很多个版本。\n 你显然难以完整记得自己改了哪些地方。\n 因为一些原因，你想要把这个文件夹恢复到某个历史版本，Git 能帮你做到：\n\r\u0026ldquo;Git 哟，请把这个文件夹(或者说仓库, repository)恢复到某年某月某日我提交的那个版本啊~\u0026rdquo;\n\r\r 根据不同需求，版本回退有多种命令行实现。\n Git 就会开始翻看它的小本子，看到自己之前写的记录，想道：\n\r 那个历史版本有这些文件，文件内容是XXX... 现在的仓库内容是XYY... 现在的版本多出了 YY...，少了 XX... 根据比较结果，可以进行版本回退 于是，我把现在的这个仓库(repository，或者说文件夹)改变了回那天提交的版本.  \r\r这样，你就会看到这个文件夹里的文件和它们的内容都变回历史版本了。\nGit 分支 你可以让 Git 对同一个仓库（repository）建立很多“小本子”，它们相互独立。\n 每本小本子就是项目的一个 Git 分支\n具体实现：执行 git checkout -b \u0026lt;新分支的名字\u0026gt;，可以基于当前分支新建一个分支\n 你和你的合作者们分工，一项大任务专门用一本小本子控制版本，各自独立工作。\n 切换分支： git checkout \u0026lt;分支名\u0026gt;\n 大家都完成任务后，把本子合并起来，就把整个项目安全地往前推进了一大步。\n 合并分支： git merge\n 这种相互独立的“小本子工作法”就是使用 Git 分支的工作流程，也是 Git 工作的精髓之一。\nGit 本地仓库-远程仓库 仓库可以建立在本地（本地仓库），也可以建立在服务器上（远程仓库）。\n你可以把一些本子放到网络服务器上，更新服务器上的仓库。\n 具体实现：执行 git push\n 也可以把网上的“小本子”克隆到本地，从而能同步本地的仓库。\n 具体实现：根据情境，使用 git pull 或 git clone\n  远程分支(remote branch)：这些放在网络服务器上的小本子 远程仓库(remote repository)：储存在网上、使用这些远程分支进行“版本控制”的文件夹(repository)  About GitHub GitHub 是由一批优秀程序员搭建起来的 Git 远程仓库的免费托管平台。\n只需要在 GitHub 网站进行简单账户设置，用 Git 命令让它和本地的 Git 客户端关联起来，就可以实现在线版本管理、同步甚至多人协作的功能，而不用自己搭建服务器、建立远程仓库了，岂不美哉？\nAbout GitHub Pages GitHub Pages 是 GitHub 的一个功能，可以帮助你搭建自己的展示网站，原理是:\n 读取你发布到 GitHub 上的特定远程仓库 把里面的文件渲染成对应的网页 用 GitHub 自己的服务器帮你把网页搭建在以你的用户名作为一部分命名的网址上( 例如 username.github.io/ )  现在你对整个原理应该有大致理解了~\n","date":1521992549,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1521992549,"objectID":"ac54ed6b2552e7aec92a68fdad782a9f","permalink":"/post/a-sketch-of-git-theory/","publishdate":"2018-03-25T23:42:29+08:00","relpermalink":"/post/a-sketch-of-git-theory/","section":"post","summary":"What is Git, and how about Github and Github Pages?","tags":["Git","ABC"],"title":"A Sketch of Git Theory","type":"post"}]